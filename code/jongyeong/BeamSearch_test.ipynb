{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470d33b6-de53-4fbf-b018-ea2becfb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from checkpoint import (\n",
    "    default_checkpoint,\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    init_tensorboard,\n",
    "    write_tensorboard,\n",
    ")\n",
    "from psutil import virtual_memory\n",
    "\n",
    "from flags import Flags\n",
    "from utils import get_network, get_optimizer\n",
    "from dataset import dataset_loader, START, PAD,load_vocab\n",
    "from scheduler import CircularLRBeta\n",
    "\n",
    "from metrics import word_error_rate,sentence_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a863c894-6ca7-4132-a6a4-8881cf4f5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59cd50f-de12-4374-b357-e546d1b55b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "        \"-c\",\n",
    "        \"--config_file\",\n",
    "        dest=\"config_file\",\n",
    "        default=\"configs/SATRN.yaml\",\n",
    "        type=str,\n",
    "        help=\"Path of configuration file\",\n",
    "    )\n",
    "parser = parser.parse_args(args=[])\n",
    "\n",
    "options = Flags(parser.config_file).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4642cebf-bb22-4363-8573-4d25d0972889",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "hardware = \"cuda\" if is_cuda else \"cpu\"\n",
    "device = torch.device(hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6fd2fc-1dc4-43d1-9792-3031bc62abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = (\n",
    "        load_checkpoint(options.checkpoint, cuda=is_cuda)\n",
    "        if options.checkpoint != \"\"\n",
    "        else default_checkpoint\n",
    "    )\n",
    "model_checkpoint = checkpoint[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf59dff6-2464-40d0-9a1b-16c9f998c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transforms.Compose(\n",
    "        [\n",
    "            # Resize so all images have the same size\n",
    "            transforms.Resize((options.input_size.height, options.input_size.width)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_data_loader, validation_data_loader, train_dataset, valid_dataset = dataset_loader(options, transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c330a5ed-af03-4228-b83c-26ff3709ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/opt/ml/input/data/train_dataset/images/train_77821.jpg', 'truth': {'text': '\\\\therefore \\\\overline { B H } = 3 + 3 = 6 \\\\left( k m \\\\right)', 'encoded': [0, 191, 196, 224, 237, 166, 213, 180, 111, 108, 111, 180, 15, 229, 169, 149, 137, 1]}, 'image': tensor([[[0.7569, 0.7608, 0.7608,  ..., 0.6941, 0.6941, 0.6902],\n",
      "         [0.7529, 0.7569, 0.7647,  ..., 0.7020, 0.6902, 0.6941],\n",
      "         [0.7529, 0.7608, 0.7569,  ..., 0.6941, 0.6902, 0.6980],\n",
      "         ...,\n",
      "         [0.7647, 0.7686, 0.7765,  ..., 0.6667, 0.6667, 0.6667],\n",
      "         [0.7647, 0.7686, 0.7765,  ..., 0.6745, 0.6627, 0.6588],\n",
      "         [0.7647, 0.7686, 0.7725,  ..., 0.6706, 0.6667, 0.6627]]])}\n"
     ]
    }
   ],
   "source": [
    "data_ex = next(iter(train_dataset))\n",
    "print(data_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e1c5fa-eb3f-4654-b697-ef3a3f0887ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "input = data[\"image\"].cuda()\n",
    "expected = data[\"truth\"][\"encoded\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679ba39b-3ffe-4f8d-99ec-8548adcc7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 1, 128, 128])\n",
      "['\\\\left( \\\\sqrt \\\\left[ n \\\\right] { a } \\\\right) ^ { m } = \\\\sqrt \\\\left[ n \\\\right] { a ^ { m } }', '= \\\\frac { F _ { f } } { 2 }', '5 6 - 4 2 = 1 4', '4 9 \\\\times \\\\frac { 3 } { 7 } =', '2 \\\\times \\\\left( - 2 \\\\right) + 3', '4 \\\\times 1 0 ^ { 1 0 }', '\\\\therefore \\\\angle x = 4 5 ^ { \\\\circ }', '1 8 . 6 \\\\times 4 + 7 \\\\times 2', 'a ^ { 2 } + b ^ { 2 } = 2 1 , a - b = 3', '\\\\alpha = \\\\frac { \\\\sum \\\\tau _ { e x t } } { I } = \\\\frac { 2 \\\\left( m _ { f } - m _ { d } \\\\right) g \\\\cos \\\\theta } { l \\\\left[ \\\\left( M / 3 \\\\right) + m _ { f } + m _ { d } \\\\right] }', 'x = \\\\frac { - \\\\left( b \\\\right) \\\\pm \\\\sqrt { \\\\left( b \\\\right) ^ { 2 } - 4 a c } } { 2 a }', '\\\\overline { P _ { 1 } P _ { 2 } }', '\\\\frac { 3 \\\\sqrt { 3 } } { 4 }', '\\\\frac { 5 } { 1 2 } \\\\times 4 =', '{ C H } = h \\\\tan 3 2 ^ { \\\\circ } \\\\left( { m } \\\\right)', '\\\\left( x - 4 \\\\right) \\\\left( x - 8 \\\\right) = 0', 'n \\\\to \\\\infty', 'x = - \\\\frac { 5 } { 3 }', ', m x - y + m + 1 = 0', '1 - 0 . 9 7 7 2 = 0 . 2 2 8', 'y = a x ^ { 2 } + b x + c', '6 4 \\\\div 8 =', '{ } _ { n - 1 } P _ { r } = \\\\frac { \\\\left( n - 1 \\\\right) ! } { \\\\left( n - 1 - r \\\\right) ! } ,', '\\\\left| z _ { 1 } + z _ { 2 } \\\\right|', '3 \\\\times 4 = 1 2', '\\\\therefore y = 2 x - 4', '4 \\\\sqrt { 5 } c m', 'a ^ { 3 } - b ^ { 3 }', '\\\\int _ { a } ^ { b } \\\\left| v \\\\left( t \\\\right) \\\\right| d t', 'f \\\\left( x ^ { n } \\\\right) = n f \\\\left( x \\\\right)', 'y = a \\\\left( x ^ { 2 } + 4 x + 4 \\\\right) - 6', 'V = \\\\int _ { 0 } ^ { 2 } S \\\\left( x \\\\right) d x', 'y - z = 1 0 k', 'D = \\\\left( - 1 \\\\right) ^ { 2 } - 4 \\\\times \\\\frac { 1 } { 4 } \\\\times 1 = 0', 'x = \\\\frac { - b \\\\pm \\\\sqrt { b ^ { 2 } - 4 a c } } { 2 a }', 'F _ { \\\\theta f } = m g \\\\cos \\\\theta = 1 7']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(data['truth']['text'])\n",
    "print(len(data['truth']['text'][0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1eb1b3-5c86-4a61-8364-507bbbcf3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 229,  13,  ...,  -1,  -1,  -1],\n",
      "        [  0, 180,  82,  ...,  -1,  -1,  -1],\n",
      "        [  0,  70,  15,  ...,  -1,  -1,  -1],\n",
      "        ...,\n",
      "        [  0,  96, 180,  ...,  -1,  -1,  -1],\n",
      "        [  0, 204, 180,  ...,  -1,  -1,  -1],\n",
      "        [  0, 178,  10,  ...,  -1,  -1,  -1]], device='cuda:0')\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(expected)\n",
    "print(len(expected[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92188fbf-e62c-4603-a505-3bc427d10ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected[expected == -1] = train_data_loader.dataset.token_to_id[PAD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60812a5d-0320-43a9-97ac-8e53f7ba5456",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d36213-f1ea-4817-aa51-16b52a2f0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from dataset import START, PAD\n",
    "from networks.SATRN import TransformerEncoderFor2DFeatures, TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd4a6a0-dbe1-4efd-9d55-6727e3ed1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=options.data.rgb\n",
    "hidden_dim=options.SATRN.encoder.hidden_dim\n",
    "filter_size=options.SATRN.encoder.filter_dim\n",
    "head_num=options.SATRN.encoder.head_num\n",
    "layer_num=options.SATRN.encoder.layer_num\n",
    "dropout_rate=options.dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e5fb73-018d-4aca-9391-6bff3d7f731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderFor2DFeatures(\n",
       "  (shallow_cnn): DeepCNN300(\n",
       "    (conv0): Conv2d(1, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (block1): DenseBlock(\n",
       "      (block): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(48, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(120, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(168, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (6): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (7): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(216, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (8): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(240, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (9): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(264, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (10): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (11): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(312, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (12): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(336, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (13): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(360, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (14): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (15): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(408, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans1): TransitionBlock(\n",
       "      (norm): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(432, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (block2): DenseBlock(\n",
       "      (block): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(216, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(240, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(264, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(312, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(336, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (6): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(360, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (7): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (8): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(408, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (9): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(432, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (10): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(456, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(456, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (11): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (12): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(504, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (13): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(528, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (14): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(552, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(552, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (15): BottleneckBlock(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(72, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trans2_norm): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (trans2_relu): ReLU(inplace=True)\n",
       "    (trans2_conv): Conv2d(600, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding2D(\n",
       "    (h_linear): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (w_linear): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (k_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (v_linear): Linear(in_features=300, out_features=296, bias=True)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (out_linear): Linear(in_features=296, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward_layer): Feedforward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (feedforward_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoderFor2DFeatures(input_size, hidden_dim, filter_size, head_num, layer_num, dropout_rate)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb5521e-cca5-4c51-b326-83b03b5aa433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 256, 300])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_result = encoder(input)\n",
    "encoder_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17deb30-9f56-419b-9106-aa38eaffb251",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33938893-7740-4b9f-915d-ddf356988c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from networks.SATRN import PositionEncoder1D, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "811ae578-664d-499d-9dcf-3f396368be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(train_dataset.id_to_token)\n",
    "src_dim=options.SATRN.decoder.src_dim\n",
    "hidden_dim=options.SATRN.decoder.hidden_dim\n",
    "filter_dim=options.SATRN.decoder.filter_dim\n",
    "head_num=options.SATRN.decoder.head_num\n",
    "dropout_rate=options.dropout_rate\n",
    "pad_id=train_dataset.token_to_id[PAD]\n",
    "st_id=train_dataset.token_to_id[START]\n",
    "layer_num=options.SATRN.decoder.layer_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3661eb6-7592-4add-b6e4-6d8f973bc988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(246, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_classes + 1, hidden_dim)\n",
    "embedding.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "920bd158-8269-48be-982d-602b2d4f897f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionEncoder1D(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoder = PositionEncoder1D(\n",
    "            in_channels=hidden_dim, dropout=dropout_rate\n",
    "        )\n",
    "pos_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdc1ebfa-4f1b-4be6-b107-dc1bf640f59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): TransformerDecoderLayer(\n",
       "    (self_attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (feedforward_layer): Feedforward(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (feedforward_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (1): TransformerDecoderLayer(\n",
       "    (self_attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (feedforward_layer): Feedforward(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (feedforward_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (2): TransformerDecoderLayer(\n",
       "    (self_attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (attention_layer): MultiHeadAttention(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (feedforward_layer): Feedforward(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (feedforward_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(\n",
    "                    hidden_dim, src_dim, filter_dim, head_num, dropout_rate\n",
    "                )\n",
    "                for _ in range(layer_num)\n",
    "            ]\n",
    "        )\n",
    "attention_layers.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e01eae2c-26df-47ae-8dbe-2a8a1ccfbd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=245, bias=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = nn.Linear(hidden_dim, num_classes)\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f65955-9a68-4467-b987-2bed8608865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = pad_id\n",
    "st_id = st_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baf29936-fd49-44d1-bda8-1cf083d9cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(text):\n",
    "        pad_mask = text == pad_id\n",
    "        pad_mask[:, 0] = False\n",
    "        pad_mask = pad_mask.unsqueeze(1)\n",
    "\n",
    "        return pad_mask\n",
    "\n",
    "def order_mask(length):\n",
    "        order_mask = torch.triu(torch.ones(length, length), diagonal=1).bool()\n",
    "        order_mask = order_mask.unsqueeze(0).to(device)\n",
    "        return order_mask\n",
    "\n",
    "def text_embedding(texts):\n",
    "        tgt = embedding(texts)\n",
    "        tgt *= math.sqrt(tgt.size(2))\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478f8ad-dbba-49d2-9b81-636caa91d95c",
   "metadata": {},
   "source": [
    "## Beam Search decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4f6411f-c5e3-4282-aac0-895581a8edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34e8fc13-2d90-4819-95ab-54bd2e9354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, src_batch, previousNode, target_batch, features, logProb, length):\n",
    "        self.src_batch = src_batch\n",
    "        self.prevNode = previousNode\n",
    "        self.target_batch = target_batch\n",
    "        self.features = features\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "        \n",
    "    def eval(self, alpha = 1.0):\n",
    "        reward = 0\n",
    "        \n",
    "        return self.logp / float(self.leng - 1 +1e6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e35399e2-0d34-4ce5-a571-1002d9156f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = encoder_result\n",
    "text = expected[:, :-1]\n",
    "is_train = False\n",
    "batch_max_length = 10\n",
    "teacher_forcing_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3431d058-fcd7-42ae-8def-0bcac3755f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 3\n",
    "topk = 1  # how many sentence do you want to generate\n",
    "decoded_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b2aa2e78-9464-4125-ab62-6f08b5c85c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,  49,  69,  69,  70,  53, 164,  41, 137]],\n",
       "\n",
       "        [[  0, 217, 239, 138, 124, 150, 193,  73, 105]],\n",
       "\n",
       "        [[  0,   5, 143, 230,  71,  26,  98, 214, 180]],\n",
       "\n",
       "        [[  0,   5,  15, 230, 228, 121, 143,  81,  29]],\n",
       "\n",
       "        [[  0, 105,  78,  26,  98, 214, 109,  29, 213]],\n",
       "\n",
       "        [[  0, 173, 236, 120, 157, 193, 140, 236,  26]],\n",
       "\n",
       "        [[  0, 217, 236, 143, 193, 175,  29,  29,  29]],\n",
       "\n",
       "        [[  0,   5, 228, 121, 143, 195, 175,  29,  29]],\n",
       "\n",
       "        [[  0, 236,  26, 224,  26,  98, 180, 208, 121]],\n",
       "\n",
       "        [[  0,   5,  32, 200, 164,  41,  26, 206, 171]],\n",
       "\n",
       "        [[  0, 236,  29, 121, 143, 195, 175, 180,  98]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 230,  71, 236,  29,  29]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 193, 143, 193,  45,  58]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 193,  45, 112, 146, 121]],\n",
       "\n",
       "        [[  0,  49,  71, 236, 143, 195, 175, 180, 208]],\n",
       "\n",
       "        [[  0,   5, 193, 236, 171, 102, 108, 236,  26]],\n",
       "\n",
       "        [[  0,   5,  32, 180, 208, 121, 143, 230, 228]],\n",
       "\n",
       "        [[  0,  49,  71, 236,  29, 236,  29,  29,  29]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 230, 228, 143, 193, 143]],\n",
       "\n",
       "        [[  0,   5, 143, 193, 143, 193, 143, 193, 143]],\n",
       "\n",
       "        [[  0, 217, 239, 193, 140, 236,  29,  91, 236]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 193, 180,  29, 180, 208]],\n",
       "\n",
       "        [[  0,   5,  45,  58, 111, 173, 236, 143,  96]],\n",
       "\n",
       "        [[  0,   5, 140, 236,  29, 213,  29,  29, 213]],\n",
       "\n",
       "        [[  0,   5, 214, 180, 208, 147,  26,   2,   2]],\n",
       "\n",
       "        [[  0, 236, 143, 193, 140, 236, 143,  26,  98]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 195, 175,  29, 213,  97]],\n",
       "\n",
       "        [[  0,  49, 230, 228, 143, 193, 180, 171,  98]],\n",
       "\n",
       "        [[  0,   5, 228, 179, 180, 208, 102, 108, 235]],\n",
       "\n",
       "        [[  0, 217, 236,  41,  26, 224,  26, 224,  29]],\n",
       "\n",
       "        [[  0,   5, 143, 195, 175, 180,  98, 180, 208]],\n",
       "\n",
       "        [[  0, 100, 236,  29, 236, 143, 198,  29,  29]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 193, 140, 236,  29,  29]],\n",
       "\n",
       "        [[  0,   5,  32, 180, 243, 109,  29, 213,  29]],\n",
       "\n",
       "        [[  0, 243, 120, 207, 100, 236,  29,  29, 213]],\n",
       "\n",
       "        [[  0,   5, 228, 143, 230,  71,  26,   2,   2]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each sentence\n",
    "for idx in range(src.size(0)):\n",
    "    \n",
    "    out = []\n",
    "    num_steps = batch_max_length - 1\n",
    "    target = torch.LongTensor(src.size(0)).fill_(st_id).to(device) # [START] token\n",
    "    features = [None] * layer_num\n",
    "    \n",
    "    target_batch = target[idx].unsqueeze(-1)\n",
    "    target_batch = target_batch.unsqueeze(-1)\n",
    "    src_batch = src[idx,:,:].unsqueeze(0)\n",
    "    \n",
    "    endnodes = []\n",
    "    number_required = 3\n",
    "    \n",
    "    node = BeamSearchNode(src_batch, None, target_batch, features, 0, 1)\n",
    "    nodes = PriorityQueue()\n",
    "     \n",
    "    nodes.put((-node.eval(), node))\n",
    "    flag = 0\n",
    "    \n",
    "    # each step \n",
    "    for t in range(num_steps):\n",
    "        \n",
    "        if flag == 1:\n",
    "            break\n",
    "            \n",
    "        nextnodes = []\n",
    "        \n",
    "        # ecah beam search candidate\n",
    "        while nodes.queue != []:\n",
    "            \n",
    "            score, n = nodes.get()\n",
    "            target_batch = n.target_batch\n",
    "            src_batch = n.src_batch\n",
    "            #features = n.features\n",
    "            if n.features[0] != None:\n",
    "                features = []\n",
    "                for i in range(len(n.features)):\n",
    "                    with torch.no_grad():\n",
    "                        features.append(copy.deepcopy(n.features[i].detach()))\n",
    "            else:\n",
    "                features = n.features\n",
    "                \n",
    "            tgt = text_embedding(target_batch)\n",
    "            tgt = pos_encoder(tgt, point=t)\n",
    "            tgt_mask = order_mask(t + 1)\n",
    "            tgt_mask = tgt_mask[:, -1].unsqueeze(1) \n",
    "            \n",
    "            if n.target_batch.item() == 1 and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    flag = 1\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            # each attention layer \n",
    "            for l, layer in enumerate(attention_layers):\n",
    "                tgt = attention_layers[l](tgt, features[l], src_batch, tgt_mask)\n",
    "                features[l] = (tgt if features[l] == None else torch.cat([features[l], tgt], 1))\n",
    "            \n",
    "            _out = generator(tgt)\n",
    "            log_prob, indexes = torch.topk(_out, beam_width)\n",
    "            \n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][0][new_k].view(1, -1)\n",
    "                log_p = log_prob[0][0][new_k].item()\n",
    "\n",
    "                node = BeamSearchNode(src_batch, n, decoded_t, features, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "            \n",
    "        if flag == 0:\n",
    "            #sorting nextnode\n",
    "            sorted(nextnodes, key=operator.itemgetter(0))\n",
    "            # put them into queue\n",
    "            for i in range(beam_width):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "        \n",
    "    \n",
    "    if len(endnodes) == 0:\n",
    "        endnodes = [nodes.get() for _ in range(1)]\n",
    "    \n",
    "    utterances = []\n",
    "    for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "        utterance = []\n",
    "        utterance.append(n.target_batch)\n",
    "        # back trace\n",
    "        while n.prevNode != None:\n",
    "            n = n.prevNode\n",
    "            utterance.append(n.target_batch)\n",
    "\n",
    "        utterance = utterance[:0:-1]\n",
    "        utterances.append(utterance)\n",
    "        \n",
    "    \n",
    "    seq_len = len(utterances[0])\n",
    "    if num_steps - seq_len > 0:\n",
    "        for i in range(num_steps - seq_len):\n",
    "            utterances[0].append(torch.LongTensor(1).fill_(2).to(device))\n",
    "    #print(len(utterances[0]))\n",
    "    decoded_batch.append(utterances)\n",
    "    #print(decoded_batch[-1])\n",
    "torch.tensor(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39facd6a-b800-4f95-ad4d-492020a16241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
